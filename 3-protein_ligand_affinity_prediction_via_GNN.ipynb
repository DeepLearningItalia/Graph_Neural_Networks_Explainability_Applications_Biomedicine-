{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Author: Andrea Mastropietro © All rights reserved\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import Linear, GraphConv, global_add_pool\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.utils import create_edge_index, ChemicalDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Working on device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and affinity information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affinity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10gs</th>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11gs</th>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13gs</th>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16pk</th>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184l</th>\n",
       "      <td>4.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      affinity\n",
       "10gs      6.40\n",
       "11gs      5.82\n",
       "13gs      4.62\n",
       "16pk      5.22\n",
       "184l      4.72"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = 'data/interaction_affinity_data/'\n",
    "interaction_affinities = None\n",
    "\n",
    "with open(DATA_PATH + 'interaction_affinities.json', 'r') as fp:\n",
    "    interaction_affinities = json.load(fp)\n",
    "\n",
    "affinities_df = pd.DataFrame.from_dict(interaction_affinities, orient='index', columns=['affinity'])\n",
    "\n",
    "display(affinities_df.head())\n",
    "\n",
    "affinities_df = affinities_df.sort_values(by = \"affinity\", ascending=True)\n",
    "interaction_affinities = affinities_df.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_interaction_dict = None\n",
    "num_node_features = 0\n",
    "\n",
    "descriptors_interaction_dict = {}\n",
    "descriptors_interaction_dict[\"CA\"] = [1, 0, 0, 0, 0, 0, 0, 0]\n",
    "descriptors_interaction_dict[\"NZ\"] = [0, 1, 0, 0, 0, 0, 0, 0]\n",
    "descriptors_interaction_dict[\"N\"] = [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "descriptors_interaction_dict[\"OG\"] = [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "descriptors_interaction_dict[\"O\"] = [0, 0, 0, 0, 1, 0, 0, 0]\n",
    "descriptors_interaction_dict[\"CZ\"] = [0, 0, 0, 0, 0, 1, 0, 0]\n",
    "descriptors_interaction_dict[\"OD1\"] = [0, 0, 0, 0, 0, 0, 1, 0]\n",
    "descriptors_interaction_dict[\"ZN\"] = [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "num_node_features = len(descriptors_interaction_dict[\"CA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pli_dataset_dict(data_path):\n",
    "\n",
    "    directory = os.fsencode(data_path)\n",
    "\n",
    "    dataset_dict = {}\n",
    "    dirs = os.listdir(directory)\n",
    "    for file in tqdm(dirs):\n",
    "        interaction_name = os.fsdecode(file)\n",
    "\n",
    "        if interaction_name in interaction_affinities:\n",
    "            if os.path.isdir(data_path + interaction_name):\n",
    "                dataset_dict[interaction_name] = {}\n",
    "                G = None\n",
    "                with open(data_path + interaction_name + \"/\" + interaction_name + \"_interaction_graph.json\", 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    G = nx.Graph()\n",
    "\n",
    "                    for node in data['nodes']:\n",
    "                        G.add_node(node[\"id\"], atom_type=node[\"attype\"], origin=node[\"pl\"]) \n",
    "\n",
    "                    for edge in data['edges']:\n",
    "                        if edge[\"id1\"] != None and edge[\"id2\"] != None:\n",
    "                            G.add_edge(edge[\"id1\"], edge[\"id2\"], weight= float(edge[\"length\"]))\n",
    "                            \n",
    "\n",
    "                    for node in data['nodes']:\n",
    "                        nx.set_node_attributes(G, {node[\"id\"]: node[\"attype\"]}, \"atom_type\")\n",
    "                        nx.set_node_attributes(G, {node[\"id\"]: node[\"pl\"]}, \"origin\")\n",
    "\n",
    "                    \n",
    "                    \n",
    "                dataset_dict[interaction_name][\"networkx_graph\"] = G\n",
    "                edge_index, edge_weight = create_edge_index(G, weighted=True)\n",
    "\n",
    "                dataset_dict[interaction_name][\"edge_index\"] = edge_index\n",
    "                dataset_dict[interaction_name][\"edge_weight\"] = edge_weight\n",
    "                \n",
    "\n",
    "                num_nodes = G.number_of_nodes()\n",
    "                \n",
    "                \n",
    "                \n",
    "                dataset_dict[interaction_name][\"x\"] = torch.zeros((num_nodes, num_node_features), dtype=torch.float)\n",
    "                for node in G.nodes:\n",
    "                    dataset_dict[interaction_name][\"x\"][node] = torch.tensor(descriptors_interaction_dict[G.nodes[node][\"atom_type\"]], dtype=torch.float)\n",
    "                    \n",
    "                ## gather label\n",
    "                dataset_dict[interaction_name][\"y\"] = torch.FloatTensor([interaction_affinities[interaction_name][\"affinity\"]])\n",
    "\n",
    "    \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a313a73bd847bb966df16dfca5f15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pli_dataset_dict = generate_pli_dataset_dict(DATA_PATH + \"/dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling edge weights (distance in Angstrom - Å)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb85619590d4d15aef320b7a7206aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_level = [pli_dataset_dict[key][\"edge_weight\"] for key in pli_dataset_dict]\n",
    "second_level = [item for sublist in first_level for item in sublist]\n",
    "\n",
    "transformer = RobustScaler().fit(np.array(second_level).reshape(-1, 1))\n",
    "\n",
    "for key in tqdm(pli_dataset_dict):\n",
    "    scaled_weights = transformer.transform(np.array(pli_dataset_dict[key][\"edge_weight\"]).reshape(-1, 1))\n",
    "    scaled_weights = [x[0] for x in scaled_weights]\n",
    "    pli_dataset_dict[key][\"edge_weight\"] = torch.FloatTensor(scaled_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c18fc17c4b9444ab73a299f2c153497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_list = []\n",
    "EDGE_WEIGHT = True\n",
    "for interaction_name in tqdm(pli_dataset_dict):\n",
    "    edge_weight_sample = None\n",
    "    if EDGE_WEIGHT:\n",
    "        edge_weight_sample = pli_dataset_dict[interaction_name][\"edge_weight\"]\n",
    "    data_list.append(Data(x = pli_dataset_dict[interaction_name][\"x\"], edge_index = pli_dataset_dict[interaction_name][\"edge_index\"], edge_weight = edge_weight_sample, y = pli_dataset_dict[interaction_name][\"y\"], networkx_graph = pli_dataset_dict[interaction_name][\"networkx_graph\"], interaction_name = interaction_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChemicalDataset(\".\", data_list = data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  9662\n",
      "Number of validation samples:  903\n",
      "Number of core set samples:  257\n",
      "Number of hold-out samples:  3393\n"
     ]
    }
   ],
   "source": [
    "train_interactions = []\n",
    "val_interactions = []\n",
    "core_set_interactions = []\n",
    "hold_out_interactions = []\n",
    "\n",
    "with open(DATA_PATH + \"data_splits/training_set.csv\", 'r') as f:\n",
    "    train_interactions = f.readlines()\n",
    "\n",
    "train_interactions = [interaction.strip() for interaction in train_interactions]\n",
    "\n",
    "with open(DATA_PATH + \"data_splits/validation_set.csv\", 'r') as f:\n",
    "    val_interactions = f.readlines()\n",
    "\n",
    "val_interactions = [interaction.strip() for interaction in val_interactions]\n",
    "\n",
    "with open(DATA_PATH + \"data_splits/core_set.csv\", 'r') as f:\n",
    "    core_set_interactions = f.readlines()\n",
    "\n",
    "core_set_interactions = [interaction.strip() for interaction in core_set_interactions]\n",
    "\n",
    "with open(DATA_PATH + \"data_splits/hold_out_set.csv\", 'r') as f:\n",
    "    hold_out_interactions = f.readlines()\n",
    "\n",
    "hold_out_interactions = [interaction.strip() for interaction in hold_out_interactions]\n",
    "\n",
    "train_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in train_interactions]\n",
    "val_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in val_interactions]\n",
    "core_set_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in core_set_interactions]\n",
    "hold_out_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in hold_out_interactions]\n",
    "\n",
    "rng = np.random.default_rng(seed = SEED)\n",
    "rng.shuffle(train_data)\n",
    "rng.shuffle(val_data)\n",
    "rng.shuffle(core_set_data)\n",
    "rng.shuffle(hold_out_data)\n",
    "\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(val_data))\n",
    "print(\"Number of core set samples: \", len(core_set_data))\n",
    "print(\"Number of hold-out samples: \", len(hold_out_data))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "core_set_loader = DataLoader(core_set_data, batch_size=BATCH_SIZE)\n",
    "hold_out_loader = DataLoader(hold_out_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GNN model - GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GC_GNN(torch.nn.Module):\n",
    "    def __init__(self, node_features_dim, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(node_features_dim, hidden_channels, aggr='max')\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels, aggr='max')\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels, aggr='max')\n",
    "        self.conv4 = GraphConv(hidden_channels, hidden_channels, aggr='max')\n",
    "        self.conv5 = GraphConv(hidden_channels, hidden_channels, aggr='max')\n",
    "        self.conv6 = GraphConv(hidden_channels, hidden_channels, aggr='max')\n",
    "        self.conv7 = GraphConv(hidden_channels, hidden_channels, aggr='max')\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight = None):\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight = edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight = edge_weight))\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_weight = edge_weight))\n",
    "        x = F.relu(self.conv4(x, edge_index, edge_weight = edge_weight))\n",
    "        x = F.relu(self.conv5(x, edge_index, edge_weight = edge_weight))\n",
    "        x = F.relu(self.conv6(x, edge_index, edge_weight = edge_weight))\n",
    "        x = self.conv7(x, edge_index, edge_weight = edge_weight)\n",
    "        \n",
    "        x = global_add_pool(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GC_GNN(node_features_dim = dataset[0].x.shape[1], num_classes = 1, hidden_channels=256).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "    \n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch, edge_weight = data.edge_weight)  # Perform a single forward pass.\n",
    "            \n",
    "            loss = torch.sqrt(criterion(torch.squeeze(out), data.y))  # Compute the loss.\n",
    "        \n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    sum_loss = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = model(data.x, data.edge_index, data.batch, edge_weight = data.edge_weight)  \n",
    "        \n",
    "        if  data.y.shape[0] == 1:\n",
    "            loss = torch.sqrt(criterion(torch.squeeze(out, 1), data.y))\n",
    "        else:\n",
    "            loss = torch.sqrt(criterion(torch.squeeze(out), data.y)) * data.y.shape[0]\n",
    "        sum_loss += loss.item()\n",
    "        \n",
    "    return sum_loss / len(loader.dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388275741d7044f8beb483d64af53243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train RMSE: 2.0754, Val RMSE: 2.1572\n",
      "Epoch: 001, Train RMSE: 1.9837, Val RMSE: 2.0637\n",
      "Epoch: 002, Train RMSE: 1.8993, Val RMSE: 1.9833\n",
      "Epoch: 003, Train RMSE: 1.8570, Val RMSE: 1.9218\n",
      "Epoch: 004, Train RMSE: 1.8598, Val RMSE: 1.9150\n",
      "Epoch: 005, Train RMSE: 1.8208, Val RMSE: 1.8557\n",
      "Epoch: 006, Train RMSE: 1.7732, Val RMSE: 1.8406\n",
      "Epoch: 007, Train RMSE: 1.7464, Val RMSE: 1.8101\n",
      "Epoch: 008, Train RMSE: 1.7407, Val RMSE: 1.8078\n",
      "Epoch: 009, Train RMSE: 1.7134, Val RMSE: 1.7679\n",
      "Epoch: 010, Train RMSE: 1.6805, Val RMSE: 1.7406\n",
      "Epoch: 011, Train RMSE: 1.6835, Val RMSE: 1.7471\n",
      "Epoch: 012, Train RMSE: 1.6684, Val RMSE: 1.7225\n",
      "Epoch: 013, Train RMSE: 1.6537, Val RMSE: 1.7174\n",
      "Epoch: 014, Train RMSE: 1.6857, Val RMSE: 1.7308\n",
      "Epoch: 015, Train RMSE: 1.6117, Val RMSE: 1.6947\n",
      "Epoch: 016, Train RMSE: 1.6117, Val RMSE: 1.6928\n",
      "Epoch: 017, Train RMSE: 1.6335, Val RMSE: 1.7052\n",
      "Epoch: 018, Train RMSE: 1.6518, Val RMSE: 1.6989\n",
      "Epoch: 019, Train RMSE: 1.5921, Val RMSE: 1.6918\n",
      "Epoch: 020, Train RMSE: 1.6210, Val RMSE: 1.6951\n",
      "Epoch: 021, Train RMSE: 1.5744, Val RMSE: 1.6563\n",
      "Epoch: 022, Train RMSE: 1.5725, Val RMSE: 1.6612\n",
      "Epoch: 023, Train RMSE: 1.5693, Val RMSE: 1.6789\n",
      "Epoch: 024, Train RMSE: 1.5963, Val RMSE: 1.6986\n",
      "Epoch: 025, Train RMSE: 1.5517, Val RMSE: 1.6492\n",
      "Epoch: 026, Train RMSE: 1.5444, Val RMSE: 1.6703\n",
      "Epoch: 027, Train RMSE: 1.5071, Val RMSE: 1.6526\n",
      "Epoch: 028, Train RMSE: 1.5373, Val RMSE: 1.6524\n",
      "Epoch: 029, Train RMSE: 1.5310, Val RMSE: 1.6550\n",
      "Epoch: 030, Train RMSE: 1.5402, Val RMSE: 1.6817\n",
      "Epoch: 031, Train RMSE: 1.5089, Val RMSE: 1.6639\n",
      "Epoch: 032, Train RMSE: 1.5107, Val RMSE: 1.6715\n",
      "Epoch: 033, Train RMSE: 1.5070, Val RMSE: 1.6698\n",
      "Epoch: 034, Train RMSE: 1.5328, Val RMSE: 1.7133\n",
      "Epoch: 035, Train RMSE: 1.5540, Val RMSE: 1.7368\n",
      "Epoch: 036, Train RMSE: 1.5586, Val RMSE: 1.7274\n",
      "Epoch: 037, Train RMSE: 1.4723, Val RMSE: 1.6937\n",
      "Epoch: 038, Train RMSE: 1.4606, Val RMSE: 1.6543\n",
      "Epoch: 039, Train RMSE: 1.4701, Val RMSE: 1.7005\n",
      "Epoch: 040, Train RMSE: 1.4427, Val RMSE: 1.6968\n",
      "Epoch: 041, Train RMSE: 1.4714, Val RMSE: 1.6960\n",
      "Epoch: 042, Train RMSE: 1.4565, Val RMSE: 1.7174\n",
      "Epoch: 043, Train RMSE: 1.4532, Val RMSE: 1.7001\n",
      "Epoch: 044, Train RMSE: 1.4369, Val RMSE: 1.6602\n",
      "Epoch: 045, Train RMSE: 1.4341, Val RMSE: 1.6912\n",
      "Epoch: 046, Train RMSE: 1.4233, Val RMSE: 1.6598\n",
      "Epoch: 047, Train RMSE: 1.3984, Val RMSE: 1.6704\n",
      "Epoch: 048, Train RMSE: 1.4133, Val RMSE: 1.6972\n",
      "Epoch: 049, Train RMSE: 1.4056, Val RMSE: 1.6990\n",
      "Epoch: 050, Train RMSE: 1.4451, Val RMSE: 1.7022\n",
      "Epoch: 051, Train RMSE: 1.4011, Val RMSE: 1.6698\n",
      "Epoch: 052, Train RMSE: 1.4253, Val RMSE: 1.7258\n",
      "Epoch: 053, Train RMSE: 1.3772, Val RMSE: 1.6974\n",
      "Epoch: 054, Train RMSE: 1.4243, Val RMSE: 1.6865\n",
      "Epoch: 055, Train RMSE: 1.5020, Val RMSE: 1.7797\n",
      "Epoch: 056, Train RMSE: 1.4886, Val RMSE: 1.7349\n",
      "Epoch: 057, Train RMSE: 1.3963, Val RMSE: 1.6982\n",
      "Epoch: 058, Train RMSE: 1.4437, Val RMSE: 1.6652\n",
      "Epoch: 059, Train RMSE: 1.3798, Val RMSE: 1.7140\n",
      "Epoch: 060, Train RMSE: 1.3508, Val RMSE: 1.7612\n",
      "Epoch: 061, Train RMSE: 1.3438, Val RMSE: 1.6932\n",
      "Epoch: 062, Train RMSE: 1.3252, Val RMSE: 1.7003\n",
      "Epoch: 063, Train RMSE: 1.3257, Val RMSE: 1.7445\n",
      "Epoch: 064, Train RMSE: 1.3514, Val RMSE: 1.7828\n",
      "Epoch: 065, Train RMSE: 1.3137, Val RMSE: 1.7251\n",
      "Epoch: 066, Train RMSE: 1.3291, Val RMSE: 1.7487\n",
      "Epoch: 067, Train RMSE: 1.3277, Val RMSE: 1.7251\n",
      "Epoch: 068, Train RMSE: 1.3459, Val RMSE: 1.7357\n",
      "Epoch: 069, Train RMSE: 1.3441, Val RMSE: 1.7407\n",
      "Epoch: 070, Train RMSE: 1.3602, Val RMSE: 1.8032\n",
      "Epoch: 071, Train RMSE: 1.4570, Val RMSE: 1.8308\n",
      "Epoch: 072, Train RMSE: 1.3797, Val RMSE: 1.7923\n",
      "Epoch: 073, Train RMSE: 1.5531, Val RMSE: 1.9217\n",
      "Epoch: 074, Train RMSE: 1.5114, Val RMSE: 1.8477\n",
      "Epoch: 075, Train RMSE: 1.3191, Val RMSE: 1.7096\n",
      "Epoch: 076, Train RMSE: 1.3533, Val RMSE: 1.7604\n",
      "Epoch: 077, Train RMSE: 1.3777, Val RMSE: 1.7676\n",
      "Epoch: 078, Train RMSE: 1.3200, Val RMSE: 1.7286\n",
      "Epoch: 079, Train RMSE: 1.3692, Val RMSE: 1.7956\n",
      "Epoch: 080, Train RMSE: 1.4087, Val RMSE: 1.7990\n",
      "Epoch: 081, Train RMSE: 1.3445, Val RMSE: 1.8031\n",
      "Epoch: 082, Train RMSE: 1.3610, Val RMSE: 1.7866\n",
      "Epoch: 083, Train RMSE: 1.3687, Val RMSE: 1.7598\n",
      "Epoch: 084, Train RMSE: 1.3826, Val RMSE: 1.7671\n",
      "Epoch: 085, Train RMSE: 1.3214, Val RMSE: 1.7708\n",
      "Epoch: 086, Train RMSE: 1.4361, Val RMSE: 1.8036\n",
      "Epoch: 087, Train RMSE: 1.3941, Val RMSE: 1.7628\n",
      "Epoch: 088, Train RMSE: 1.2987, Val RMSE: 1.7546\n",
      "Epoch: 089, Train RMSE: 1.3238, Val RMSE: 1.7485\n",
      "Epoch: 090, Train RMSE: 1.3221, Val RMSE: 1.7767\n",
      "Epoch: 091, Train RMSE: 1.2906, Val RMSE: 1.7869\n",
      "Epoch: 092, Train RMSE: 1.2917, Val RMSE: 1.7981\n",
      "Epoch: 093, Train RMSE: 1.3208, Val RMSE: 1.7519\n",
      "Epoch: 094, Train RMSE: 1.2783, Val RMSE: 1.7871\n",
      "Epoch: 095, Train RMSE: 1.3181, Val RMSE: 1.7928\n",
      "Epoch: 096, Train RMSE: 1.3211, Val RMSE: 1.8599\n",
      "Epoch: 097, Train RMSE: 1.2720, Val RMSE: 1.8246\n",
      "Epoch: 098, Train RMSE: 1.3327, Val RMSE: 1.8567\n",
      "Epoch: 099, Train RMSE: 1.2899, Val RMSE: 1.8144\n",
      "Best model at epoch: 025\n",
      "Best val loss:  1.6491729656063177\n",
      "Core set RMSE with best model: 1.7349\n",
      "Hold-out set RMSE with best model: 1.6395\n"
     ]
    }
   ],
   "source": [
    "best_epoch = 0\n",
    "best_val_loss = 100000\n",
    "\n",
    "MODEL_SAVE_FOLDER = \"models/\"\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train()\n",
    "    train_rmse = test(train_loader)\n",
    "    val_rmse = test(val_loader)\n",
    "    if val_rmse < best_val_loss:\n",
    "        best_val_loss = val_rmse\n",
    "        best_epoch = epoch\n",
    "        \n",
    "        if not os.path.exists(MODEL_SAVE_FOLDER):\n",
    "            os.makedirs(MODEL_SAVE_FOLDER)\n",
    "\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_FOLDER + \"gc_gnn_model.ckpt\")\n",
    "        \n",
    "    print(f'Epoch: {epoch:03d}, Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}')\n",
    "\n",
    "\n",
    "print(f'Best model at epoch: {best_epoch:03d}')\n",
    "print(\"Best val loss: \", best_val_loss)\n",
    "\n",
    "model = GC_GNN(node_features_dim = dataset[0].x.shape[1], num_classes = 1, hidden_channels=256).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_FOLDER + \"gc_gnn_model.ckpt\"))\n",
    "model.to(device)\n",
    "\n",
    "core_set_rmse = test(core_set_loader)    \n",
    "print(f'Core set RMSE with best model: {core_set_rmse:.4f}')\n",
    "\n",
    "hold_out_set_rmse = test(hold_out_loader)    \n",
    "print(f'Hold-out set RMSE with best model: {hold_out_set_rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN_XAI_Biomedicine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
